{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Product Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to try to reproduce the amazon recommendation system which is one of amazon's best features. \n",
    "To achive the goal, I got data from both UCSD data set and also I used selenium to get scrape product information from amazon's website.\n",
    "The score I used is the cosine similarity score, and based on the the data we have, our model perform better for content based reccmendation system rather than user based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on item numbers I have from existing dataset, I analyzed the data based on user ratings for a user based recommender.\n",
    "\n",
    "After I get scraped information from amazon, I cleaned the data and did NLP for product category, brand, name and ratings.\n",
    "\n",
    "I tried to cluster the result in several ways(categories and TFIDF matrix) the clusting does not provide good results. After several trials the one with user based recommender and the data with NLP not clusting performce better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the Dataset I originally got was a huge dataset around 3GB. \n",
    " - in 1st notebook I chopped the dataset to several small ones for faster processing. Based on the scraping speed and my process capacity, I choose to process for the year 2018 first.\n",
    " - The second notebook is a single scraper for selinium as it works comparitively slow, I did the cleaning and modeling while scraping.\n",
    " - The 3rd notebook contains the content based recommender and user based recommender\n",
    " - the last one is a potencial try, as I tried to upload the dataset to sql, while dataset has larger than 1600 columns which excedding postgresql's limit, would try in a seperate time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
